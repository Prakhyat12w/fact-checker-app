# AI Fact-Checking Web App

An AI-powered web application that extracts factual claims from uploaded PDF documents, verifies them against live web data, and classifies each claim as **Verified**, **Inaccurate**, or **False** with concise reasoning and sources.

This tool acts as a **fact-checking layer before publication**, helping identify outdated statistics, incorrect claims, and unsupported assertions in reports, articles, or research documents.

---

## ğŸš€ Features

- ğŸ“„ **PDF Upload** â€“ Drag-and-drop interface for uploading documents
- ğŸ§  **Claim Extraction** â€“ Identifies non-trivial, check-worthy factual claims
- ğŸŒ **Live Web Verification** â€“ Cross-checks claims using real-time web search
- âœ… **Claim Classification**
  - Verified
  - Inaccurate (outdated or mismatched data)
  - False (no supporting evidence)
- ğŸ“Š **Clear Results Display** â€“ Each claim shown with verdict and reasoning
- â˜ï¸ **Fully Deployed** â€“ Accessible via a public URL (Streamlit Cloud)

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|------|-----------|
| Frontend | Streamlit |
| Backend | Python |
| LLM | Groq (LLaMA 3.1 â€“ 8B Instant) |
| Orchestration | LangChain |
| Web Search | Tavily API |
| PDF Parsing | pdfplumber |
| Deployment | Streamlit Cloud |

---

## ğŸ§  How It Works

1. **Upload PDF**
   - User uploads a document via the web interface.

2. **Text Extraction**
   - Text is extracted from the PDF using `pdfplumber`.

3. **Claim Extraction**
   - An LLM extracts only **non-trivial, verifiable claims** (e.g., statistics, prices, events).
   - Obvious facts (dates, calendar truths, definitions) are intentionally ignored.

4. **Live Verification**
   - Each claim is searched on the live web using Tavily.
   - Relevant evidence is collected.

5. **Classification**
   - Claims are classified as **Verified**, **Inaccurate**, or **False**.
   - A short justification is generated for each verdict.

6. **Results Display**
   - Claims and verdicts are displayed clearly in the UI.

---

## ğŸ“ Project Structure

fact-checker-app/
â”‚
â”œâ”€â”€ app.py # Streamlit entry point
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore # Git ignore rules
â”‚
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ pdf_loader.py # PDF text extraction
â”‚ â”œâ”€â”€ claim_extractor.py # Claim identification logic
â”‚ â”œâ”€â”€ verifier.py # Web verification via Tavily
â”‚ â””â”€â”€ classifier.py # Claim classification logic
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ prompts.py # LLM prompt templates
â”‚ â””â”€â”€ helpers.py # Utility functions

## âš™ï¸ Setup & Installation (Local)

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/fact-checker-app.git
cd fact-checker-app
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```
### 3. Install Dependencies
pip install -r requirements.txt

### 4. Environment Variables

Create a .env file (do not commit this):

GROQ_API_KEY=your_groq_api_key
TAVILY_API_KEY=your_tavily_api_key

### 5. Run the App
streamlit run app.py


Open http://localhost:8501 in your browser.

### The app is deployed using Streamlit Cloud.

Deployment Steps

Push the code to GitHub (excluding .env)

Go to https://share.streamlit.io

Create a new app from the repository

Set app.py as the entry point

Add secrets in App Settings â†’ Secrets:

GROQ_API_KEY = "gsk_..."
TAVILY_API_KEY = "tvly_..."


### ğŸ¥ Demo

A short demo video demonstrates:

Uploading a PDF

Extracting claims

Flagging incorrect or outdated information

(Demo video link provided in submission)

### ğŸ“Œ Evaluation Criteria Alignment

This project satisfies all required criteria:

âœ… Live, deployed web app

âœ… Extracts verifiable claims from PDFs

âœ… Uses live web data for verification

âœ… Flags false and outdated information

âœ… Clean codebase with documentation

âœ… Public URL for immediate testing

### ğŸ”’ Security & Best Practices

API keys are never committed

.env is excluded via .gitignore

Secrets are injected securely at runtime

### ğŸ“œ License

This project is intended for educational and evaluation purposes.

### ğŸ™Œ Acknowledgements

Groq for fast, free LLM inference
Tavily for reliable real-time web search
Streamlit for rapid UI deployment
LangChain for orchestration
